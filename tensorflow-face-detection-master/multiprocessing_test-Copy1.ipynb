{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ec9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 19:06:23.848974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "from __future__ import division\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import imutils\n",
    "import numpy as np\n",
    "import dlib \n",
    "import datetime\n",
    "import math \n",
    "import re\n",
    "from google.cloud import speech\n",
    "import pyaudio\n",
    "from six.moves import queue\n",
    "import random\n",
    "from multiprocessing import Process, Value\n",
    "import multiprocessing as mp\n",
    "from videoRecogntion import aaa  # faceDetect 해주는 함수 import\n",
    "from gptTest import main2, kogpt2  # stt, 키워드 추출, 문장 생성\n",
    "\n",
    "# GUI\n",
    "from box import Box  # GUI 카메랄 가져오는 용\n",
    "from pathlib import Path\n",
    "from PIL import ImageTk\n",
    "import PIL.Image\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # warning 문구 제거 \n",
    "\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=r\"/Users/yujeong/Downloads/gradstt-ac34c3b32796.json\"  # Google STT API 계정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed9c419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ce981e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8506a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# font = cv2.FONT_ITALIC\n",
    "\n",
    "# # 개체가 포함된 이미지 영역을 가져와 개체의 포즈를 정의하는 점 위치 집합을 출력하기 위한 predictor  --> 눈 인식에 사용\n",
    "# predictor = dlib.shape_predictor(\"/Users/yujeong/Desktop/졸프용/shape_predictor_68_face_landmarks.dat\")\n",
    "# tDetector = TensoflowFaceDector(PATH_TO_CKPT)  #  얼굴인식 Detector\n",
    "\n",
    "# # 표정 인식 모델 경로\n",
    "# emotion_model_path = '/Users/yujeong/Downloads/_mini_XCEPTION_model_korean_64_2.hdf5'\n",
    "\n",
    "# EMOTIONS = [\"angry\", \"scared\", \"happy\", \"sad\", \"surprised\", \"neutral\"] # 6가지 표정\n",
    "# ratio=[0,0,0,0,0,0] # 시각화를 위한 표정 비율\n",
    "# eye_fix=0 # 눈 흔들림 시각화를 위한 cnt\n",
    "# max_boxes_to_draw = 5  # 최대 ㅐ5명까지만 인식할 것\n",
    "\n",
    "# # 발표가 끝난 후 표정 평균을 내기 위함\n",
    "# people_emo = [[0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0],\n",
    "#              [0, 0, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4370f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "from google.cloud import speech\n",
    "\n",
    "import pyaudio\n",
    "from six.moves import queue\n",
    "\n",
    "# 오디오 레코딩 매개변수\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms\n",
    "\n",
    "f=open(\"stt.txt\",'w')  # 전체 발화 써주는 용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f58e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 19:06:37.053795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-21 19:06:37.732156: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # 공유 메모리를 이용한 멀티 프로세싱        \n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    \n",
    "    \n",
    "    video_pause = Value('i',0)  # 발표 종료 시 \n",
    "    \n",
    "            \n",
    "    \"\"\" 화면 구성 \"\"\"\n",
    "    \n",
    "    win=tk.Tk()\n",
    "    win.title(\"뚝딱\")\n",
    "    win.geometry(\"1000x1000\")\n",
    "    win.configure(background='white')\n",
    "\n",
    "    mainFrm=tk.Frame(win)\n",
    "    resFrm=tk.Frame(win)\n",
    "\n",
    "    mainFrm.grid(row=0,column=0,sticky='nsew')\n",
    "    resFrm.grid(row=0,column=0,sticky='nsew')\n",
    "    \n",
    "    # 메인 화면\n",
    "\n",
    "    mf1=Frame(mainFrm)\n",
    "    mf1.pack(side=\"left\")\n",
    "    \n",
    "    # 키워드 result 보여줄 화면\n",
    "    mf2_1=Frame(mainFrm,background='white')\n",
    "    mf2_1.pack()\n",
    "    \n",
    "    \n",
    "    # 꼬리질문 result 보여줄 화면\n",
    "    mf2_3=Frame(mainFrm,background='white')\n",
    "    mf2_3.pack()\n",
    "\n",
    "    \n",
    "    \"\"\" 버튼 함수 \"\"\"\n",
    "    \n",
    "    def openFrame(frame): # open 함수 \n",
    "        frame.tkraise()\n",
    "\n",
    "    \n",
    "    def keyEvent(): # 키워드 추출 가시화 함수 \n",
    "        f = open('/Users/yujeong/Desktop/keywords.txt', 'r')\n",
    "        s2 = f.readline()        \n",
    "        keyText = str(s2)\n",
    "        keyword_res.config(text=keyText)\n",
    "\n",
    "\n",
    "    def event():  # 발표 함수 \n",
    "        strtBtn['text']='발표중'\n",
    "        main()\n",
    "\n",
    "    def exitEvent(): # 발표 끝 함수\n",
    "        video_pause.value = 1   # 공유변수를 1로 수정함 --> 다른 파일에서 공유 후 기능을 멈춤\n",
    "        \n",
    "    \n",
    "    def tailEvent(): # 꼬리질문 가시화 함수 \n",
    "        f3 = open('/Users/yujeong/Desktop/question.txt', 'r')\n",
    "        s3 = f3.readline()\n",
    "        tailText = str(s3)\n",
    "        tail_res.config(text = tailText)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 결과 화면에서 이미지 넘기는 함수들 \n",
    "    \n",
    "    def next_img1():\n",
    "        global img_num1\n",
    "        img_num1+=1\n",
    "        if img_num1>=len(img_list):\n",
    "            img_num1=0\n",
    "        canvas1.itemconfig(can_img1,image=img_list[img_num1])\n",
    "\n",
    "    def pre_img1():\n",
    "        global img_num1\n",
    "        img_num1-=1\n",
    "        if img_num1<0:\n",
    "            img_num1=len(img_list)-1\n",
    "        canvas1.itemconfig(can_img1,image=img_list[img_num1])\n",
    "\n",
    "    def next_img2():\n",
    "        global img_num2\n",
    "        img_num2+=1\n",
    "        if img_num2>=len(eye_list):\n",
    "            img_num2=0\n",
    "        canvas2.itemconfig(can_img2,image=eye_list[img_num2])\n",
    "\n",
    "    def pre_img2():\n",
    "        global img_num2\n",
    "        img_num2-=1\n",
    "        if img_num2<0:\n",
    "            img_num2=len(eye_list)-1\n",
    "        canvas2.itemconfig(can_img2,image=eye_list[img_num2])    \n",
    "    \n",
    "    \n",
    "    # 웹캠\n",
    "    video = Box(mf1, height=600,width=700)\n",
    "    video.show_frames()  # Show the created Box\n",
    "\n",
    "\n",
    "\n",
    "    # 글자 및 버튼 GUI 구성\n",
    "    \n",
    "    keyword=Label(mf2_1, text='추출 keyword') # fg는 글자 색 지정, font로 글자 설정\n",
    "    keyword.place(x=140, y=10)\n",
    "    keyword.pack()\n",
    "    \n",
    "    keyword_res=Label(mf2_1, width=28,height=10,relief='solid',wraplength=200,text=' ') # wraplength=줄바꿈 단위, relief=테두리 설정\n",
    "    keyword_res.place(x=100, y=40)\n",
    "    keyword_res.pack()\n",
    "    \n",
    "    kwBtn=Button(mf2_1,text='키워드 추출하기',padx=10,pady=10,command=keyEvent)\n",
    "    kwBtn.pack()\n",
    "\n",
    "    tail=Label(mf2_3, text='꼬리질문') # fg는 글자 색 지정, font로 글자 설정\n",
    "    tail.place(x=140, y=10)\n",
    "    tail.pack()\n",
    "    \n",
    "    tail_res=Label(mf2_3, width=28,height=10,relief='solid',wraplength=200,text=' ') # wraplength=줄바꿈 단위, relief=테두리 설정\n",
    "    tail_res.place(x=100, y=40)\n",
    "    tail_res.pack()\n",
    "    \n",
    "    tailBtn=Button(mf2_3,text='꼬리질문 생성하기',padx=10,pady=10,command=tailEvent)\n",
    "    tailBtn.pack()\n",
    "     \n",
    "    mf3=Frame(mainFrm)\n",
    "    mf3.pack()\n",
    "\n",
    "    strtBtn=Button(mf3,text='발표 시작',padx=10,pady=10,command=event)\n",
    "    endBtn=Button(mf3,text='발표 종료',padx=10,pady=10,command=exitEvent)\n",
    "    # 다음페이지로 화면 전환\n",
    "    btnToRes=Button(mf3,text='결과 보기',padx=10,pady=10,command=lambda:[openFrame(resFrm)])  \n",
    "\n",
    "    strtBtn.pack(side=\"left\")\n",
    "    btnToRes.pack(side=\"right\")\n",
    "    endBtn.pack(side=\"right\")\n",
    "    \n",
    "    \n",
    "    # 결과 화면\n",
    "    \n",
    "    # 표정 결과\n",
    "    path = os.path.join(\"/Users/yujeong/Desktop/CNN_test/tensorflow-face-detection-master/save_fig/save_emotion/\")\n",
    "    file_list_emotion = os.listdir(path)\n",
    "    del file_list_emotion[0]\n",
    "    file_list_emotion.sort()\n",
    "\n",
    "    img_list = []\n",
    "    for file in file_list_emotion[:5]:\n",
    "        file_path = path + file\n",
    "        img = PIL.Image.open(file_path)\n",
    "        img = img.resize((int(img.width / 2), int(img.height / 2)))\n",
    "        img_list.append(ImageTk.PhotoImage(img))\n",
    "\n",
    "    # 눈 결과\n",
    "    path = os.path.join(\"/Users/yujeong/Desktop/CNN_test/tensorflow-face-detection-master/save_fig/save_eye/\")\n",
    "    file_list_eye = os.listdir(path)\n",
    "    del file_list_eye[0]\n",
    "    file_list_eye.sort()\n",
    "\n",
    "    eye_list = []\n",
    "    for file in file_list_eye[:5]:\n",
    "        file_path = path + file\n",
    "        img = PIL.Image.open(file_path)\n",
    "        img = img.resize((int(img.width / 2), int(img.height / 2)))\n",
    "        eye_list.append(ImageTk.PhotoImage(img))\n",
    "\n",
    "    # 발화 속도 결과 \n",
    "    img = PIL.Image.open('/Users/yujeong/Desktop/CNN_test/tensorflow-face-detection-master/save_fig/save_speed/speed.png')\n",
    "    img = img.resize((int(img.width / 3), int(img.height / 3)))\n",
    "    speed_img=ImageTk.PhotoImage(img)\n",
    "\n",
    "\n",
    "    rf1=Frame(resFrm)\n",
    "    rf1.pack()\n",
    "\n",
    "    faceDetect=Label(rf1, text='표정인식 결과') # fg는 글자 색 지정, font로 글자 설정\n",
    "    faceDetect.place(x=10, y=10)\n",
    "    faceDetect.pack()\n",
    "\n",
    "    forward_btn=Button(rf1,text=\"이전\",command=pre_img1)\n",
    "    forward_btn.place(x=5, y=10)\n",
    "    forward_btn.pack(side=\"left\")\n",
    "\n",
    "    backward_btn=Button(rf1,text=\"다음\",command=next_img1)\n",
    "    backward_btn.place(x=15, y=10)\n",
    "    backward_btn.pack(side=\"right\")\n",
    "\n",
    "\n",
    "    canvas1 = Canvas(rf1,width=230,height=230,bg='black')\n",
    "    canvas1.place(x=20, y=50)\n",
    "    canvas1.pack()\n",
    "\n",
    "    img_num1 = 0\n",
    "    can_img1=canvas1.create_image(120,115,image=img_list[img_num1])\n",
    "\n",
    "    # 시선 처리 결과\n",
    "    rf3=Frame(resFrm)\n",
    "    rf3.pack()\n",
    "\n",
    "    eye_move=Label(rf3, text='발화속도 결과') # fg는 글자 색 지정, font로 글자 설정\n",
    "    eye_move.place(x=10, y=10)\n",
    "    eye_move.pack()\n",
    "\n",
    "    forward_btn=Button(rf3,text=\"이전\",command=pre_img2)\n",
    "    forward_btn.place(x=5, y=10)\n",
    "    forward_btn.pack(side=\"left\")\n",
    "\n",
    "    backward_btn=Button(rf3,text=\"다음\",command=next_img2)\n",
    "    backward_btn.place(x=15, y=10)\n",
    "    backward_btn.pack(side=\"right\")\n",
    "\n",
    "\n",
    "    canvas2 = Canvas(rf3,width=230,height=180)\n",
    "    canvas2.place(x=50, y=100)\n",
    "    canvas2.pack()\n",
    "\n",
    "    img_num2 = 0\n",
    "    can_img2=canvas2.create_image(115,100,image=eye_list[img_num2])\n",
    "\n",
    "    # 발화 속도 결과\n",
    "    rf2=Frame(resFrm)\n",
    "    rf2.pack()\n",
    "\n",
    "    speed=Label(rf2, text='시선처리 결과') # fg는 글자 색 지정, font로 글자 설정\n",
    "    speed.place(x=10, y=0)\n",
    "    speed.pack()\n",
    "\n",
    "    canvas3 = Canvas(rf1,width=320,height=200)\n",
    "    canvas3.place(x=50, y=30)\n",
    "    canvas3.pack()\n",
    "\n",
    "    canvas3.create_image(125,100,image=speed_img)\n",
    "\n",
    "    # 버튼\n",
    "    rf4=Frame(resFrm)\n",
    "    rf4.pack()\n",
    "\n",
    "    btnToMain=Button(rf4,text=\"메인으로 돌아가기\",padx=10,pady=10,command=lambda:[openFrame(mainFrm)])\n",
    "\n",
    "    btnToMain.pack()\n",
    "    \n",
    "    openFrame(mainFrm)\n",
    "\n",
    "\n",
    "    \n",
    "    ###################\n",
    "    \"\"\"멀티 프로세싱\"\"\"\n",
    "    \n",
    "    # 프로세스 생성 후 시작\n",
    "    \n",
    "    # 표정인식\n",
    "    face_func = Process(target=aaa, args=(video_pause, ))\n",
    "    face_func.start()\n",
    "\n",
    "    # 키워드 추출, 질문 생성, 발화속도\n",
    "    nlp_func = Process(target=main2, args=(video_pause, ))\n",
    "    nlp_func.start()\n",
    "    \n",
    "    # GUI 돌리기 용\n",
    "    loop_func = Process(target=win.mainloop())\n",
    "    loop_func.start()\n",
    "\n",
    "    # 메서드는 join() 메서드가 호출된 프로세스가 종료될 때까지 블록\n",
    "    face_func.join()\n",
    "    nlp_func.join()\n",
    "    loop_func.join()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
